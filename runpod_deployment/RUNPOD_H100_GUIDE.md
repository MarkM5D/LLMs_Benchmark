# ğŸš€ RunPod H100 Benchmark Ã‡alÄ±ÅŸtÄ±rma Rehberi

## ğŸ“‹ AdÄ±m AdÄ±m RunPod H100 KullanÄ±mÄ±

### 1ï¸âƒ£ **RunPod Instance BaÅŸlatma**

1. **RunPod'a Login Olun**: https://runpod.io
2. **GPU Cloud** â†’ **Deploy** 
3. **Template SeÃ§in**: 
   - `PyTorch` veya `CUDA` template seÃ§in
   - Veya `RunPod Pytorch` template'ini kullanÄ±n
4. **GPU SeÃ§in**: 
   - **H100 80GB** kartÄ±nÄ± seÃ§in (en gÃ¼Ã§lÃ¼ seÃ§enek)
   - Fiyat: ~$4-5/saat
5. **Deploy** butonuna basÄ±n

### 2ï¸âƒ£ **Pod'a BaÄŸlanma**

Pod hazÄ±r olunca:
1. **"Connect"** â†’ **"Start Web Terminal"** 
2. Terminal aÃ§Ä±lacak, ÅŸu komutla baÅŸlayÄ±n:

```bash
cd /workspace
pwd  # Åu anda /workspace'de olmalÄ±sÄ±nÄ±z
```

### 3ï¸âƒ£ **Kod Upload Etme YÃ¶ntemleri**

#### **YÃ¶ntem A: Git Clone (Ã–nerilen)**
```bash
# EÄŸer GitHub'a push ettiyseniz:
git clone https://github.com/[kullanÄ±cÄ±-adÄ±nÄ±z]/[repo-adÄ±nÄ±z].git
cd [repo-adÄ±nÄ±z]
```

#### **YÃ¶ntem B: Manual Upload**
1. RunPod File Manager kullanÄ±n
2. `runpod_deployment/` klasÃ¶rÃ¼nÃ¼ `/workspace/` altÄ±na upload edin
3. Terminal'de:
```bash
cd /workspace/runpod_deployment
ls -la  # DosyalarÄ± kontrol edin
```

#### **YÃ¶ntem C: Wget (EÄŸer zip dosyasÄ± varsa)**
```bash
cd /workspace
wget [zip-dosyasÄ±-linki]
unzip [dosya-adÄ±].zip
cd [klasÃ¶r-adÄ±]
```

### 4ï¸âƒ£ **H100 Environment Setup**

```bash
# Ã‡alÄ±ÅŸtÄ±rma izinleri verin
chmod +x setup_env.sh
chmod +x runpod_startup.sh

# Otomatik H100 setup (TEK KOMUT!)
./runpod_startup.sh
```

Bu komut ÅŸunlarÄ± yapacak:
- H100 Hopper mimarisi ayarlarÄ±
- PyTorch, vLLM, SGLang, TensorRT-LLM kurulumu
- Dataset download
- GPU optimizasyonlarÄ±

### 5ï¸âƒ£ **H100 Verification**

```bash
# H100 kartÄ±nÄ± kontrol edin
nvidia-smi

# H100 optimizasyonlarÄ±nÄ± doÄŸrulayÄ±n
python3 h100_optimize.py
```

**Beklenen Ã‡Ä±ktÄ±:**
```
ğŸ® GPU: NVIDIA H100 80GB HBM3
ğŸ”¢ Compute Capability: 9.0
âœ… H100 GPU detected!
âœ… Hopper architecture (9.0+) confirmed
```

### 6ï¸âƒ£ **Benchmark Ã‡alÄ±ÅŸtÄ±rma**

```bash
# Ana benchmark scripti (3 engine'i test eder)
python3 run_all.py

# Veya tek tek Ã§alÄ±ÅŸtÄ±rmak iÃ§in:
python3 benchmarks/vllm_benchmark.py
python3 benchmarks/sglang_benchmark.py  
python3 benchmarks/tensorrtllm_benchmark.py

# SonuÃ§larÄ± aggregate etmek iÃ§in:
python3 aggregate_results.py
```

### 7ï¸âƒ£ **SonuÃ§larÄ± GÃ¶rÃ¼ntÃ¼leme**

#### **Real-time Monitoring**
```bash
# AyrÄ± terminal aÃ§Ä±n, GPU kullanÄ±mÄ±nÄ± izleyin
watch -n 1 nvidia-smi

# Memory ve CPU monitoring
htop
```

#### **Benchmark SonuÃ§larÄ±**
```bash
# SonuÃ§ dosyalarÄ±nÄ± listeleyin
ls -la benchmarks/results/

# JSON sonuÃ§larÄ±nÄ± pretty print ile gÃ¶rÃ¼n
python3 -m json.tool benchmarks/results/vllm_results.json

# HÄ±zlÄ± Ã¶zet gÃ¶rÃ¼ntÃ¼leme
cat benchmarks/results/benchmark_summary.json | jq '.'
```

#### **Grafik SonuÃ§lar**
```bash
# EÄŸer matplotlib kuruluysa grafik oluÅŸtur
python3 -c "
import json
import matplotlib.pyplot as plt

# Throughput comparison grafiÄŸi
with open('benchmarks/results/aggregate_results.json', 'r') as f:
    data = json.load(f)

engines = []
throughputs = []
for engine, results in data.get('engine_comparison', {}).items():
    engines.append(engine)
    throughputs.append(results.get('throughput_tokens_per_second', 0))

plt.figure(figsize=(10, 6))
plt.bar(engines, throughputs, color=['blue', 'green', 'red'])
plt.title('H100 LLM Engine Throughput Comparison')
plt.ylabel('Tokens per Second')
plt.savefig('benchmarks/results/throughput_comparison.png', dpi=150, bbox_inches='tight')
print('ğŸ“Š Grafik kaydedildi: benchmarks/results/throughput_comparison.png')
"
```

### 8ï¸âƒ£ **SonuÃ§larÄ± Download Etme**

```bash
# TÃ¼m sonuÃ§larÄ± zip'leyin
cd /workspace
zip -r benchmark_results.zip runpod_deployment/benchmarks/results/

# RunPod File Manager'dan indirebilirsiniz
ls -la benchmark_results.zip
```

## ğŸ”§ **Troubleshooting KomutlarÄ±**

### **GPU Problemleri**
```bash
# GPU detection
nvidia-smi
lspci | grep -i nvidia

# CUDA version
nvcc --version
python3 -c "import torch; print(f'CUDA Available: {torch.cuda.is_available()}, Version: {torch.version.cuda}')"
```

### **Memory Problemleri**
```bash
# GPU memory temizle
python3 -c "import torch; torch.cuda.empty_cache(); print('GPU memory cleared')"

# System memory kontrol
free -h
df -h
```

### **Package Problemleri**
```bash
# Inference engine kontrolÃ¼
python3 -c "
try:
    import vllm; print(f'âœ… vLLM {vllm.__version__}')
except: print('âŒ vLLM not installed')

try:
    import sglang; print(f'âœ… SGLang {sglang.__version__}')
except: print('âŒ SGLang not installed')

try:
    import tensorrt_llm; print(f'âœ… TensorRT-LLM {tensorrt_llm.__version__}')
except: print('âŒ TensorRT-LLM not installed')
"

# Manuel reinstall
pip install --force-reinstall --no-cache-dir vllm
```

## ğŸ“Š **Beklenen H100 SonuÃ§larÄ±**

### **Tipik Performans Metrikleri:**
- **vLLM**: 8,000-12,000 tokens/sec
- **SGLang**: 7,000-10,000 tokens/sec  
- **TensorRT-LLM**: 10,000-15,000 tokens/sec
- **Latency P95**: <200ms
- **GPU Utilization**: 85-95%
- **Memory Usage**: 40-70GB (80GB H100'den)

### **BaÅŸarÄ± Ä°ndikatÃ¶rleri:**
```bash
# Bu Ã§Ä±ktÄ±larÄ± gÃ¶rmelisiniz:
âœ… H100 GPU detected!
âœ… All inference engines installed
âœ… Benchmarks completed successfully
âœ… Results saved to benchmarks/results/
```

## ğŸ¯ **Pro Tips**

### **Performance Optimization**
```bash
# Maximum performance iÃ§in
export CUDA_VISIBLE_DEVICES=0
export OMP_NUM_THREADS=8
ulimit -n 65536

# Model caching
export TRANSFORMERS_CACHE=/workspace/cache
export HF_DATASETS_CACHE=/workspace/cache
mkdir -p /workspace/cache
```

### **Cost Optimization**
```bash
# Ä°ÅŸlem bitince pod'u durdurun (Ã¼cretlendirme durur)
# Ã–nemli dosyalar /workspace'de kalÄ±r

# Log monitoring
tail -f benchmarks/logs/*.log
```

## ğŸš¨ **Acil Durumlar**

### **Pod DonmasÄ±**
1. RunPod dashboard'dan "Restart" yapÄ±n
2. `/workspace` dosyalarÄ±nÄ±z korunur
3. `cd /workspace/runpod_deployment && python3 run_all.py` ile devam edin

### **Out of Memory**
```bash
# Batch size kÃ¼Ã§Ã¼ltÃ¼n
export BENCHMARK_BATCH_SIZE=16  # Default 32 yerine
python3 run_all.py
```

Bu rehberle H100'de tam performans alacaksÄ±nÄ±z! ğŸš€âš¡