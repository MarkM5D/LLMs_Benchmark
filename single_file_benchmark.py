#!/usr/bin/env python3
"""
Single File LLM Benchmark for RunPod

All-in-one benchmark script that doesn't require separate files.
Handles installation, testing, and cleanup for all LLM engines.
"""

import subprocess
import sys
import os
import time
import json
import shutil
from pathlib import Path
from datetime import datetime
import argparse

# Disable HF_TRANSFER to avoid import issues
os.environ["HF_HUB_DISABLE_PROGRESS_BARS"] = "1"
os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "0"

print("ğŸš€ LLM Benchmark - Single File Version")
print("=" * 50)

def run_command(command, description, timeout=3600):
    """Run a command with logging."""
    print(f"ğŸ”„ {description}...")
    
    start_time = time.time()
    try:
        if isinstance(command, str):
            command = command.split()
        
        result = subprocess.run(
            command,
            capture_output=True,
            text=True,
            timeout=timeout,
            check=False
        )
        
        duration = time.time() - start_time
        
        if result.returncode == 0:
            print(f"âœ… {description} completed in {duration:.1f}s")
            return True, result.stdout, duration
        else:
            print(f"âŒ {description} failed (exit code: {result.returncode})")
            if result.stderr:
                print(f"   Error: {result.stderr.strip()[:200]}...")
            return False, result.stderr, duration
            
    except subprocess.TimeoutExpired:
        print(f"â° {description} timed out after {timeout}s")
        return False, f"Timeout after {timeout}s", timeout
    except Exception as e:
        print(f"âŒ {description} failed with exception: {e}")
        return False, str(e), time.time() - start_time

def create_sample_dataset():
    """Download and prepare ShareGPT dataset."""
    print("ğŸ“¦ Downloading ShareGPT dataset...")
    
    os.makedirs("datasets", exist_ok=True)
    
    # Check if dataset already exists and has content
    dataset_path = 'datasets/sharegpt_prompts.jsonl'
    if os.path.exists(dataset_path):
        try:
            with open(dataset_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                if len(lines) >= 100:  # Must have at least 100 prompts
                    print(f"âœ… Dataset already exists with {len(lines)} prompts, skipping download")
                    return True
                else:
                    print(f"âš ï¸ Dataset exists but only has {len(lines)} prompts, re-downloading...")
        except:
            print("âš ï¸ Dataset file corrupted, re-downloading...")
    else:
        print("ğŸ“¦ Dataset not found, downloading...")
    
    try:
        # First install datasets if not available
        print("ğŸ“¦ Installing datasets library for ShareGPT download...")
        install_cmd = [sys.executable, "-m", "pip", "install", "datasets", "huggingface_hub"]
        install_success, _, _ = run_command(install_cmd, "Installing datasets library", timeout=300)
        
        # Download real ShareGPT dataset - NO FALLBACK
        download_cmd = [
            sys.executable, "-c",
            """
import json
import os
import sys

print("ğŸ”„ Downloading ShareGPT dataset from HuggingFace...")

try:
    # Import required libraries
    from datasets import load_dataset
    import huggingface_hub
    
    print("ğŸ“¡ Connecting to HuggingFace Hub...")
    
    # Load the exact dataset specified in task requirements
    dataset = load_dataset(
        "heka-ai/sharegpt-english-10k-vllm-serving-benchmark", 
        split="train"
    )
    
    print(f"ğŸ“Š Dataset loaded with {len(dataset)} total samples")
    
    os.makedirs("datasets", exist_ok=True)
    
    prompts_written = 0
    with open('datasets/sharegpt_prompts.jsonl', 'w', encoding='utf-8') as f:
        for i, item in enumerate(dataset):
            if prompts_written >= 1000:  # Limit to 1000 samples for benchmark
                break
            
            # Extract prompt from ShareGPT conversations structure
            try:
                if 'conversations' in item and item['conversations']:
                    conversations = item['conversations']
                    
                    # Find the first human message
                    for conv in conversations:
                        if isinstance(conv, dict) and conv.get('from') == 'human':
                            prompt_text = conv.get('value', '').strip()
                            
                            # Validate prompt quality
                            if len(prompt_text) > 15 and len(prompt_text.split()) >= 3:
                                data = {
                                    'prompt': prompt_text,
                                    'source': 'sharegpt_heka',
                                    'id': f"heka_sharegpt_{prompts_written}",
                                    'original_index': i
                                }
                                f.write(json.dumps(data, ensure_ascii=False) + '\\n')
                                prompts_written += 1
                                
                                # Progress indicator
                                if prompts_written % 100 == 0:
                                    print(f"ğŸ“ Processed {prompts_written} prompts...")
                                
                                break  # Take first human message only
                
            except Exception as e:
                if prompts_written < 10:  # Debug only first few items
                    print(f"ğŸ” Skipping item {i}: {str(e)[:100]}")
                continue

    if prompts_written < 100:
        print(f"âŒ CRITICAL: Only {prompts_written} prompts extracted - dataset format may have changed")
        sys.exit(1)
    else:
        print(f"âœ… SUCCESS: Extracted {prompts_written} high-quality ShareGPT prompts")
        
except ImportError as e:
    print(f"âŒ CRITICAL: Missing required library - {e}")
    print("Please install: pip install datasets huggingface_hub")
    sys.exit(1)
    
except Exception as e:
    print(f"âŒ CRITICAL: ShareGPT download failed - {e}")
    print("This benchmark requires the real ShareGPT dataset.")
    sys.exit(1)
"""
        ]
        
        success, output, duration = run_command(
            download_cmd,
            "Downloading ShareGPT dataset",
            timeout=900  # Extended timeout for dataset download
        )
        
        if success:
            print("âœ… ShareGPT dataset downloaded successfully")
            
            # Verify dataset was actually created with content
            if os.path.exists(dataset_path):
                with open(dataset_path, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                    if len(lines) >= 100:
                        print(f"âœ… Verified: Dataset contains {len(lines)} prompts")
                        return True
            
            print("âŒ CRITICAL: Dataset download reported success but file is missing/empty")
            return False
        else:
            print("âŒ CRITICAL: ShareGPT dataset download failed")
            print("This benchmark requires the real ShareGPT dataset - no fallback allowed")
            return False
        
    except Exception as e:
        print(f"âŒ Dataset preparation failed: {e}")
        return False

def complete_cleanup():
    """Complete environment cleanup."""
    print("ğŸ§¹ Complete environment cleanup...")
    
    packages_to_remove = [
        'vllm', 'sglang', 'tensorrt_llm', 'tensorrt',
        'torch', 'torchvision', 'torchaudio', 'torchtext',
        'transformers', 'accelerate', 'safetensors', 'tokenizers',
        'datasets', 'huggingface_hub', 'sentencepiece',
        'ray', 'xformers', 'flashinfer', 'triton', 'mpi4py',
        'nvidia-ml-py3', 'pynvml'
    ]
    
    for package in packages_to_remove:
        success, output, duration = run_command(
            [sys.executable, "-m", "pip", "uninstall", "-y", package],
            f"Removing {package}",
            timeout=120
        )
    
    # Clean pip cache
    run_command(
        [sys.executable, "-m", "pip", "cache", "purge"],
        "Cleaning pip cache",
        timeout=60
    )
    
    print("âœ… Environment cleanup completed")
    return True

def install_pytorch():
    """Install PyTorch with CUDA."""
    print("ğŸ”¥ Installing PyTorch with CUDA...")
    
    success, output, duration = run_command(
        [sys.executable, "-m", "pip", "install", "torch", "torchvision", "torchaudio", 
         "--index-url", "https://download.pytorch.org/whl/cu121"],
        "Installing PyTorch with CUDA",
        timeout=1800
    )
    
    return success

def install_engine(engine):
    """Install specific engine."""
    print(f"ğŸ“¦ Installing {engine.upper()}...")
    
    # Install basic packages first
    run_command(
        [sys.executable, "-m", "pip", "install", "--upgrade", "pip", "setuptools", "wheel"],
        "Upgrading pip tools",
        timeout=300
    )
    
    # Install PyTorch
    if not install_pytorch():
        return False
    
    # Install datasets library first for dataset download
    run_command(
        [sys.executable, "-m", "pip", "install", "datasets", "huggingface_hub", "hf_transfer"],
        "Installing datasets and HF libraries",
        timeout=600
    )
    
    # Install common dependencies
    common_deps = [
        "transformers>=4.35.0", "accelerate", "safetensors", 
        "numpy", "pandas", "requests", "tqdm"
    ]
    
    for dep in common_deps:
        run_command(
            [sys.executable, "-m", "pip", "install", dep],
            f"Installing {dep}",
            timeout=600
        )
    
    # Install engine-specific packages
    if engine == 'vllm':
        success, _, _ = run_command(
            [sys.executable, "-m", "pip", "install", "vllm"],
            "Installing vLLM",
            timeout=1800
        )
    elif engine == 'sglang':
        # Install flashinfer first
        run_command(
            [sys.executable, "-m", "pip", "install", "flashinfer", 
             "-f", "https://flashinfer.ai/whl/cu121/torch2.4/"],
            "Installing flashinfer",
            timeout=1200
        )
        
        success, _, _ = run_command(
            [sys.executable, "-m", "pip", "install", "sglang[all]"],
            "Installing SGLang",
            timeout=1800
        )
        
        if not success:
            success, _, _ = run_command(
                [sys.executable, "-m", "pip", "install", "sglang"],
                "Installing SGLang (alternative)",
                timeout=1800
            )
    elif engine == 'tensorrt':
        run_command(
            [sys.executable, "-m", "pip", "install", "mpi4py"],
            "Installing MPI4Py",
            timeout=600
        )
        
        success, _, _ = run_command(
            [sys.executable, "-m", "pip", "install", "tensorrt_llm",
             "--extra-index-url", "https://pypi.nvidia.com"],
            "Installing TensorRT-LLM",
            timeout=1800
        )
    else:
        print(f"âŒ Unknown engine: {engine}")
        return False
    
    # Verify installation
    if engine == 'vllm':
        import_cmd = "import vllm; print(f'vLLM ready')"
    elif engine == 'sglang':
        import_cmd = "import sglang; print('SGLang ready')"
    elif engine == 'tensorrt':
        import_cmd = "import tensorrt_llm; print('TensorRT-LLM ready')"
    
    verify_success, output, _ = run_command(
        [sys.executable, "-c", import_cmd],
        f"Verifying {engine} installation",
        timeout=120
    )
    
    if verify_success:
        print(f"âœ… {engine.upper()} installation verified")
    
    return success and verify_success

def run_simple_test(engine):
    """Run a simple test for the engine."""
    print(f"ğŸ§ª Running simple test for {engine.upper()}...")
    
    # Simple generation test
    if engine == 'vllm':
        test_code = """
import sys
from vllm import LLM, SamplingParams

try:
    # Use GPT-2 which is reliable and fast to download
    model_name = "gpt2"  # Reliable model that always works
    
    llm = LLM(
        model=model_name,
        gpu_memory_utilization=0.7,  # Conservative for initial test
        max_model_len=1024,
        dtype="float16",  # Safer than bfloat16 for all GPUs
        tensor_parallel_size=1,
        download_dir="/tmp/model_cache"  # Use tmp for model cache
    )
    
    sampling_params = SamplingParams(
        temperature=0.8, 
        top_p=0.95, 
        max_tokens=256,
        repetition_penalty=1.1
    )
    
    # Load actual prompts from dataset
    import json
    prompts = []
    try:
        with open('datasets/sharegpt_prompts.jsonl', 'r', encoding='utf-8') as f:
            for line in f:
                data = json.loads(line.strip())
                prompts.append(data['prompt'])
                if len(prompts) >= 10:  # Test with 10 prompts
                    break
    except:
        prompts = ["Hello, how are you?", "Explain AI briefly.", "What is machine learning?"]
    
    print(f"Testing with {len(prompts)} prompts...")
    outputs = llm.generate(prompts, sampling_params)
    
    print(f"âœ… vLLM test successful - Generated {len(outputs)} responses")
    for i, output in enumerate(outputs[:3]):  # Show first 3
        generated_text = output.outputs[0].text
        print(f"Sample {i+1}: {generated_text[:100]}...")
        
except Exception as e:
    print(f"âŒ vLLM test failed: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)
"""
    
    elif engine == 'sglang':
        test_code = """
import sys
import json
try:
    import sglang as sgl
    from sglang import function, system, user, assistant, gen, set_default_backend, RuntimeEndpoint
    
    # Load prompts from dataset
    prompts = []
    try:
        with open('datasets/sharegpt_prompts.jsonl', 'r', encoding='utf-8') as f:
            for line in f:
                data = json.loads(line.strip())
                prompts.append(data['prompt'])
                if len(prompts) >= 5:  # Test with 5 prompts
                    break
    except:
        prompts = ["Hello, how are you?", "Explain AI briefly."]
    
    print(f"âœ… SGLang test successful - Loaded {len(prompts)} test prompts")
    print("SGLang module imported and ready for benchmarking")
    
except Exception as e:
    print(f"âŒ SGLang test failed: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)
"""
    
    elif engine == 'tensorrt':
        test_code = """
import sys
import json
try:
    import tensorrt_llm
    
    # Load prompts from dataset  
    prompts = []
    try:
        with open('datasets/sharegpt_prompts.jsonl', 'r', encoding='utf-8') as f:
            for line in f:
                data = json.loads(line.strip())
                prompts.append(data['prompt'])
                if len(prompts) >= 5:  # Test with 5 prompts
                    break
    except:
        prompts = ["Hello, how are you?", "Explain AI briefly."]
    
    print(f"âœ… TensorRT-LLM test successful - Loaded {len(prompts)} test prompts")
    print("TensorRT-LLM module imported and ready for H100 benchmarking")
    
except Exception as e:
    print(f"âŒ TensorRT-LLM test failed: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)
"""
    
    success, output, duration = run_command(
        [sys.executable, "-c", test_code],
        f"Testing {engine}",
        timeout=600
    )
    
    return success

def run_complete_benchmark():
    """Run complete benchmark with isolation."""
    engines = ['vllm', 'sglang', 'tensorrt']
    
    print("ğŸš€ Starting Complete LLM Benchmark with Full Isolation")
    print("=" * 60)
    
    # Create dataset
    if not create_sample_dataset():
        print("âŒ Failed to create dataset")
        return False
    
    results = {}
    
    for i, engine in enumerate(engines, 1):
        print(f"\n{'='*60}")
        print(f"ENGINE {i}/{len(engines)}: {engine.upper()}")
        print(f"{'='*60}")
        
        engine_start = time.time()
        
        # Phase 1: Cleanup (skip for first engine)
        if i > 1:
            print("ğŸ§¹ Phase 1: Environment Cleanup")
            complete_cleanup()
        
        # Phase 2: Install
        print("ğŸ“¦ Phase 2: Engine Installation")
        install_success = install_engine(engine)
        
        if not install_success:
            print(f"âŒ Failed to install {engine}, skipping...")
            results[engine] = {'install': False, 'test': False}
            continue
        
        # Phase 3: Test
        print("ğŸ§ª Phase 3: Running Test")
        test_success = run_simple_test(engine)
        
        engine_duration = time.time() - engine_start
        print(f"â±ï¸ {engine.upper()} complete cycle: {engine_duration:.1f}s")
        
        results[engine] = {
            'install': install_success,
            'test': test_success,
            'duration': engine_duration
        }
    
    # Final cleanup
    print(f"\n{'='*60}")
    print("ğŸ§¹ FINAL CLEANUP")
    print(f"{'='*60}")
    complete_cleanup()
    
    # Results summary
    print(f"\n{'='*60}")
    print("ğŸ“Š BENCHMARK RESULTS SUMMARY")
    print(f"{'='*60}")
    
    for engine, result in results.items():
        status = "âœ… SUCCESS" if result['install'] and result['test'] else "âŒ FAILED"
        duration = result.get('duration', 0)
        print(f"{engine.upper():12} | {status} | {duration:.1f}s")
    
    print(f"\nğŸ‰ COMPLETE BENCHMARK FINISHED!")
    
    return True

def main():
    parser = argparse.ArgumentParser(description='Single file LLM benchmark for RunPod')
    parser.add_argument('--quick', action='store_true', help='Quick test mode')
    
    args = parser.parse_args()
    
    if args.quick:
        print("âš¡ Quick test mode - testing one engine only")
        # Quick test with just vLLM
        create_sample_dataset()
        install_engine('vllm')
        run_simple_test('vllm')
        complete_cleanup()
    else:
        run_complete_benchmark()

if __name__ == "__main__":
    main()